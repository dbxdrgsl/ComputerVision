{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d09947b2",
   "metadata": {},
   "source": [
    "# Lab 5 – Medical image segmentation with U-Net\n",
    "\n",
    "## Tasks\n",
    "- Split dataset 70/30 (train/val)\n",
    "- Train U-Net\n",
    "- Evaluate (Pixel Accuracy, IoU, Dice)\n",
    "- Implement U-Net v2\n",
    "- Compare baseline vs v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec9461a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Imports and environment checks\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919f463",
   "metadata": {},
   "source": [
    "# Dataset scan + pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and configuration\n",
    "from pathlib import Path\n",
    "\n",
    "# Repository-relative roots (no absolute paths)\n",
    "REPO_ROOT = Path('.') .resolve()\n",
    "DATA_ROOT = REPO_ROOT / 'data' / 'brain_tumor'\n",
    "RUNS_DIR = REPO_ROOT / 'runs'\n",
    "MODELS_DIR = REPO_ROOT / 'models'\n",
    "\n",
    "# Split files\n",
    "SPLIT_TRAIN = RUNS_DIR / 'split_train.txt'\n",
    "SPLIT_TEST  = RUNS_DIR / 'split_test.txt'\n",
    "\n",
    "# Training config\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "\n",
    "# Ensure directories exist\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('REPO_ROOT:', REPO_ROOT)\n",
    "print('DATA_ROOT exists:', DATA_ROOT.exists())\n",
    "print('RUNS_DIR:', RUNS_DIR)\n",
    "print('MODELS_DIR:', MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan dataset and match image↔mask pairs\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "IMG_EXTS = {'.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "def is_img_ext(p: Path) -> bool:\n",
    "    return p.suffix.lower() in IMG_EXTS\n",
    "\n",
    "def normalize_stem(stem: str) -> str:\n",
    "    s = stem.lower()\n",
    "    # strip common mask suffixes\n",
    "    for suf in ['_mask', '-mask', 'mask', '_seg', '-seg']:\n",
    "        if s.endswith(suf):\n",
    "            s = s[: -len(suf)]\n",
    "            break\n",
    "    # strip common mask prefix\n",
    "    if s.startswith('mask_'):\n",
    "        s = s[len('mask_'):]\n",
    "    return s\n",
    "\n",
    "def scan_files(root: Path):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for p in root.rglob('*'):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        ext = p.suffix.lower()\n",
    "        if ext in IMG_EXTS:\n",
    "            # Heuristic: consider anything under 'images' as image, 'masks' as mask, else infer by name\n",
    "            name = p.stem.lower()\n",
    "            if 'mask' in name or 'seg' in name or 'masks' in p.parts:\n",
    "                masks.append(p)\n",
    "            else:\n",
    "                images.append(p)\n",
    "        # some datasets store masks as PNG while images JPEG; above handles both\n",
    "    return images, masks\n",
    "\n",
    "def rel_to_repo(p: Path) -> Path:\n",
    "    # convert to path relative to repo root\n",
    "    try:\n",
    "        return p.resolve().relative_to(REPO_ROOT.resolve())\n",
    "    except Exception:\n",
    "        # fallback: make a relative path using Path.relative_to if possible\n",
    "        return Path(*p.parts[-len(p.parts):])\n",
    "\n",
    "def match_pairs(images, masks):\n",
    "    by_stem = defaultdict(list)\n",
    "    for m in masks:\n",
    "        by_stem[normalize_stem(m.stem)].append(m)\n",
    "\n",
    "    pairs = []\n",
    "    for img in images:\n",
    "        key = normalize_stem(img.stem)\n",
    "        candidates = by_stem.get(key, [])\n",
    "        if not candidates:\n",
    "            continue\n",
    "        # prefer same-directory matches\n",
    "        same_dir = [m for m in candidates if m.parent == img.parent]\n",
    "        m = same_dir[0] if same_dir else candidates[0]\n",
    "        pairs.append((rel_to_repo(img).as_posix(), rel_to_repo(m).as_posix()))\n",
    "    return pairs\n",
    "\n",
    "# Execute scan and matching\n",
    "imgs, msks = scan_files(DATA_ROOT)\n",
    "pairs = match_pairs(imgs, msks)\n",
    "print(f'Found images: {len(imgs)} | masks: {len(msks)} | pairs: {len(pairs)}')\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    img_stems = [p.stem for p in imgs[:10]]\n",
    "    msk_stems = [p.stem for p in msks[:10]]\n",
    "    print('Sample image stems (up to 10):', img_stems)\n",
    "    print('Sample mask stems (up to 10):', msk_stems)\n",
    "    raise RuntimeError('No image-mask pairs matched. Please verify naming conventions and folder structure.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4353f7c4",
   "metadata": {},
   "source": [
    "# 70/30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c1ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministic split and write files\n",
    "import random\n",
    "\n",
    "# Ensure pairs exist (from previous cell)\n",
    "assert 'pairs' in globals(), 'pairs not defined. Run dataset pairing cell first.'\n",
    "\n",
    "# Sort then shuffle with fixed seed\n",
    "pairs_sorted = sorted(pairs)\n",
    "random.Random(42).shuffle(pairs_sorted)\n",
    "\n",
    "N = len(pairs_sorted)\n",
    "split_idx = int(0.7 * N)\n",
    "train_pairs = pairs_sorted[:split_idx]\n",
    "test_pairs  = pairs_sorted[split_idx:]\n",
    "\n",
    "# Write split files with POSIX paths: <img_path> <mask_path>\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with SPLIT_TRAIN.open('w', encoding='utf-8') as f:\n",
    "    for img, msk in train_pairs:\n",
    "        f.write(f\"{img} {msk}\\n\")\n",
    "with SPLIT_TEST.open('w', encoding='utf-8') as f:\n",
    "    for img, msk in test_pairs:\n",
    "        f.write(f\"{img} {msk}\\n\")\n",
    "\n",
    "# Diagnostics\n",
    "print(f'Total pairs: {N} | train: {len(train_pairs)} | test: {len(test_pairs)}')\n",
    "\n",
    "# Show first 3 lines from each split file\n",
    "def head_lines(path, n=3):\n",
    "    lines = []\n",
    "    try:\n",
    "        with path.open('r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= n:\n",
    "                    break\n",
    "                lines.append(line.rstrip('\\n'))\n",
    "    except FileNotFoundError:\n",
    "        lines = ['<missing>']\n",
    "    return lines\n",
    "\n",
    "print('split_train.txt (first 3):', head_lines(SPLIT_TRAIN, 3))\n",
    "print('split_test.txt  (first 3):', head_lines(SPLIT_TEST, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe8501",
   "metadata": {},
   "source": [
    "# tf.data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f54ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tf.data pipeline for loading, resizing, normalizing\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "\n",
    "assert SPLIT_TRAIN.exists() and SPLIT_TEST.exists(), 'Split files missing. Run the split cell first.'\n",
    "\n",
    "# Read split files into lists of Paths\n",
    "def read_split(path: Path):\n",
    "    pairs = []\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            img_str, mask_str = line.split(' ', 1)\n",
    "            pairs.append((Path(img_str), Path(mask_str)))\n",
    "    return pairs\n",
    "\n",
    "train_list = read_split(SPLIT_TRAIN)\n",
    "test_list  = read_split(SPLIT_TEST)\n",
    "print(f'Read split lists: train={len(train_list)} test={len(test_list)}')\n",
    "\n",
    "IMG_H, IMG_W = IMG_SIZE\n",
    "\n",
    "# Loader using tf.io and tf.image\n",
    "def load_one(img_path: tf.Tensor, mask_path: tf.Tensor):\n",
    "    img_bytes = tf.io.read_file(img_path)\n",
    "    msk_bytes = tf.io.read_file(mask_path)\n",
    "\n",
    "    img = tf.image.decode_image(img_bytes, channels=3, expand_animations=False)\n",
    "    msk = tf.image.decode_image(msk_bytes, channels=1, expand_animations=False)\n",
    "\n",
    "    # Ensure shape is fully defined after decode_image\n",
    "    img.set_shape([None, None, 3])\n",
    "    msk.set_shape([None, None, 1])\n",
    "\n",
    "    # Resize\n",
    "    img = tf.image.resize(img, [IMG_H, IMG_W], method='bilinear')\n",
    "    msk = tf.image.resize(msk, [IMG_H, IMG_W], method='nearest')\n",
    "\n",
    "    # Normalize\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    msk = tf.cast(msk > 0, tf.float32)\n",
    "    return img, msk\n",
    "\n",
    "# Build datasets\n",
    "def make_ds(pairs_list, batch=BATCH_SIZE, shuffle=False):\n",
    "    img_paths = [str(REPO_ROOT / p[0]) for p in pairs_list]\n",
    "    msk_paths = [str(REPO_ROOT / p[1]) for p in pairs_list]\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices((img_paths, msk_paths))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(pairs_list), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda ip, mp: load_one(ip, mp), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(train_list, batch=BATCH_SIZE, shuffle=True)\n",
    "test_ds  = make_ds(test_list, batch=1, shuffle=False)\n",
    "\n",
    "# Print one batch shapes\n",
    "for imgs, msks in train_ds.take(1):\n",
    "    print('Train batch shapes:', imgs.shape, msks.shape)\n",
    "for imgs, msks in test_ds.take(1):\n",
    "    print('Test batch shapes:', imgs.shape, msks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity visualization: 3 samples from train_ds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "samples = list(train_ds.unbatch().take(3))\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for i, (img, msk) in enumerate(samples):\n",
    "    img_np = img.numpy()\n",
    "    msk_np = msk.numpy().squeeze()\n",
    "    \n",
    "    # Image\n",
    "    axes[i, 0].imshow(img_np)\n",
    "    axes[i, 0].set_title(f'Sample {i+1}: Image')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Mask\n",
    "    axes[i, 1].imshow(msk_np, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Sample {i+1}: Mask')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Overlay: image + mask contour or alpha-blend\n",
    "    axes[i, 2].imshow(img_np)\n",
    "    axes[i, 2].imshow(msk_np, cmap='Reds', alpha=0.4)\n",
    "    axes[i, 2].set_title(f'Sample {i+1}: Overlay')\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "out_path = RUNS_DIR / 'dataset_check.png'\n",
    "plt.savefig(out_path, dpi=150, bbox_inches='tight')\n",
    "print(f'Saved visualization to {out_path}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcf2f0d",
   "metadata": {},
   "source": [
    "# Baseline U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline U-Net architecture using Keras\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def conv_block(x, filters):\n",
    "    \"\"\"Two consecutive Conv2D layers with ReLU activation.\"\"\"\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block(x, filters):\n",
    "    \"\"\"Encoder block: conv_block + max pooling. Returns skip connection and pooled output.\"\"\"\n",
    "    skip = conv_block(x, filters)\n",
    "    pooled = layers.MaxPooling2D(2)(skip)\n",
    "    return skip, pooled\n",
    "\n",
    "def decoder_block(x, skip, filters):\n",
    "    \"\"\"Decoder block: Conv2DTranspose (upsampling) + concatenate with skip + conv_block.\"\"\"\n",
    "    x = layers.Conv2DTranspose(filters, 2, strides=2, padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip])\n",
    "    x = conv_block(x, filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape=(256, 256, 3), base_filters=32, depth=4):\n",
    "    \"\"\"Build a U-Net model with specified input shape, base filters, and depth.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder path\n",
    "    skips = []\n",
    "    x = inputs\n",
    "    for i in range(depth):\n",
    "        skip, x = encoder_block(x, base_filters * (2 ** i))\n",
    "        skips.append(skip)\n",
    "    \n",
    "    # Bottleneck\n",
    "    x = conv_block(x, base_filters * (2 ** depth))\n",
    "    \n",
    "    # Decoder path\n",
    "    for i in reversed(range(depth)):\n",
    "        x = decoder_block(x, skips[i], base_filters * (2 ** i))\n",
    "    \n",
    "    # Output layer: single-channel binary mask\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='UNet_baseline')\n",
    "    return model\n",
    "\n",
    "# Build and summarize model\n",
    "unet = build_unet(input_shape=(*IMG_SIZE, 3), base_filters=32, depth=4)\n",
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea482c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics: IoU and Dice (TensorFlow-safe)\n",
    "import tensorflow as tf\n",
    "\n",
    "def iou_metric(y_true, y_pred, threshold=0.5, epsilon=1e-7):\n",
    "    \"\"\"Intersection over Union metric with thresholding.\"\"\"\n",
    "    y_pred_bin = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true_bin = tf.cast(y_true > threshold, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true_bin * y_pred_bin)\n",
    "    union = tf.reduce_sum(y_true_bin) + tf.reduce_sum(y_pred_bin) - intersection\n",
    "    \n",
    "    iou = (intersection + epsilon) / (union + epsilon)\n",
    "    return iou\n",
    "\n",
    "def dice_metric(y_true, y_pred, threshold=0.5, epsilon=1e-7):\n",
    "    \"\"\"Dice coefficient metric with thresholding.\"\"\"\n",
    "    y_pred_bin = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true_bin = tf.cast(y_true > threshold, tf.float32)\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true_bin * y_pred_bin)\n",
    "    dice = (2.0 * intersection + epsilon) / (tf.reduce_sum(y_true_bin) + tf.reduce_sum(y_pred_bin) + epsilon)\n",
    "    return dice\n",
    "\n",
    "print('IoU and Dice metrics defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96983cf",
   "metadata": {},
   "source": [
    "# Training baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13262619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train baseline U-Net\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compile model\n",
    "unet.compile(\n",
    "    optimizer=Adam(learning_rate=LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[iou_metric, dice_metric]\n",
    ")\n",
    "print('Model compiled.')\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path = MODELS_DIR / 'unet_best.keras'\n",
    "tb_log_dir = RUNS_DIR / 'tb' / 'baseline'\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        str(checkpoint_path),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=str(tb_log_dir),\n",
    "        histogram_freq=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f'Checkpoint: {checkpoint_path}')\n",
    "print(f'TensorBoard logs: {tb_log_dir}')\n",
    "\n",
    "# Train\n",
    "history = unet.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot and save training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history.history['loss'], label='train_loss')\n",
    "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "\n",
    "# IoU\n",
    "axes[1].plot(history.history['iou_metric'], label='train_iou')\n",
    "axes[1].plot(history.history['val_iou_metric'], label='val_iou')\n",
    "axes[1].set_title('IoU')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "\n",
    "# Dice\n",
    "axes[2].plot(history.history['dice_metric'], label='train_dice')\n",
    "axes[2].plot(history.history['val_dice_metric'], label='val_dice')\n",
    "axes[2].set_title('Dice')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "hist_path = RUNS_DIR / 'baseline_history.png'\n",
    "plt.savefig(hist_path, dpi=150, bbox_inches='tight')\n",
    "print(f'Saved training history to {hist_path}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddde3a32",
   "metadata": {},
   "source": [
    "# Evaluation baseline (lab formulas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86910b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set using lab formulas\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load best checkpoint\n",
    "best_model_path = MODELS_DIR / 'unet_best.keras'\n",
    "model_eval = load_model(str(best_model_path), custom_objects={'iou_metric': iou_metric, 'dice_metric': dice_metric})\n",
    "print(f'Loaded best model from {best_model_path}')\n",
    "\n",
    "# Collect metrics per image\n",
    "pa_list = []\n",
    "iou_list = []\n",
    "dice_list = []\n",
    "\n",
    "# Collect samples for qualitative visualization\n",
    "qual_samples = []\n",
    "\n",
    "for img_batch, gt_batch in test_ds:\n",
    "    # Predict\n",
    "    pred_batch = model_eval.predict(img_batch, verbose=0)\n",
    "    \n",
    "    # Process each image in batch (batch=1 for test_ds)\n",
    "    for img, gt, pred in zip(img_batch.numpy(), gt_batch.numpy(), pred_batch):\n",
    "        # Threshold prediction at 0.5\n",
    "        pred_bin = (pred > 0.5).astype(np.float32)\n",
    "        gt_bin = (gt > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Flatten for metric computation\n",
    "        gt_flat = gt_bin.flatten()\n",
    "        pred_flat = pred_bin.flatten()\n",
    "        \n",
    "        # Compute TP, TN, FP, FN\n",
    "        TP = np.sum((gt_flat == 1) & (pred_flat == 1))\n",
    "        TN = np.sum((gt_flat == 0) & (pred_flat == 0))\n",
    "        FP = np.sum((gt_flat == 0) & (pred_flat == 1))\n",
    "        FN = np.sum((gt_flat == 1) & (pred_flat == 0))\n",
    "        \n",
    "        # Pixel Accuracy\n",
    "        pa = (TP + TN) / (TP + TN + FP + FN + 1e-7)\n",
    "        \n",
    "        # IoU\n",
    "        iou = TP / (TP + FP + FN + 1e-7)\n",
    "        \n",
    "        # Dice\n",
    "        dice = (2 * TP) / (2 * TP + FP + FN + 1e-7)\n",
    "        \n",
    "        pa_list.append(pa)\n",
    "        iou_list.append(iou)\n",
    "        dice_list.append(dice)\n",
    "        \n",
    "        # Store first 10 samples for visualization\n",
    "        if len(qual_samples) < 10:\n",
    "            qual_samples.append((img, gt.squeeze(), pred_bin.squeeze()))\n",
    "\n",
    "# Print mean metrics\n",
    "print(f'\\n=== Baseline Evaluation (test set, n={len(pa_list)}) ===')\n",
    "print(f'Pixel Accuracy: {np.mean(pa_list):.4f} ± {np.std(pa_list):.4f}')\n",
    "print(f'IoU:            {np.mean(iou_list):.4f} ± {np.std(iou_list):.4f}')\n",
    "print(f'Dice:           {np.mean(dice_list):.4f} ± {np.std(dice_list):.4f}')\n",
    "\n",
    "# Save qualitative results: grid of 10 samples (image / GT / pred / overlay)\n",
    "eval_dir = RUNS_DIR / 'eval' / 'baseline'\n",
    "eval_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(10, 4, figsize=(16, 40))\n",
    "for i, (img, gt, pred) in enumerate(qual_samples):\n",
    "    # Image\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f'Sample {i+1}: Image')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(gt, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Sample {i+1}: GT')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Sample {i+1}: Pred')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Overlay: image + prediction\n",
    "    axes[i, 3].imshow(img)\n",
    "    axes[i, 3].imshow(pred, cmap='Reds', alpha=0.4)\n",
    "    axes[i, 3].set_title(f'Sample {i+1}: Overlay')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "qual_path = eval_dir / 'qualitative_results.png'\n",
    "plt.savefig(qual_path, dpi=150, bbox_inches='tight')\n",
    "print(f'Saved qualitative results to {qual_path}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390494d0",
   "metadata": {},
   "source": [
    "# U-Net v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4407b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net v2 with BatchNorm, Dropout, and combined BCE+Dice loss\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def conv_block_v2(x, filters, dropout=False):\n",
    "    \"\"\"Conv block with BatchNorm after each Conv2D (before activation) and optional Dropout.\"\"\"\n",
    "    x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    if dropout:\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "    return x\n",
    "\n",
    "def encoder_block_v2(x, filters, dropout=False):\n",
    "    \"\"\"Encoder block v2 with optional dropout.\"\"\"\n",
    "    skip = conv_block_v2(x, filters, dropout=dropout)\n",
    "    pooled = layers.MaxPooling2D(2)(skip)\n",
    "    return skip, pooled\n",
    "\n",
    "def decoder_block_v2(x, skip, filters):\n",
    "    \"\"\"Decoder block v2.\"\"\"\n",
    "    x = layers.Conv2DTranspose(filters, 2, strides=2, padding='same')(x)\n",
    "    x = layers.Concatenate()([x, skip])\n",
    "    x = conv_block_v2(x, filters, dropout=False)\n",
    "    return x\n",
    "\n",
    "def build_unet_v2(input_shape=(256, 256, 3), base_filters=32, depth=4):\n",
    "    \"\"\"Build U-Net v2 with BatchNorm and Dropout in bottleneck and deepest encoder.\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Encoder path\n",
    "    skips = []\n",
    "    x = inputs\n",
    "    for i in range(depth):\n",
    "        # Apply dropout to deepest encoder (i == depth-1)\n",
    "        dropout = (i == depth - 1)\n",
    "        skip, x = encoder_block_v2(x, base_filters * (2 ** i), dropout=dropout)\n",
    "        skips.append(skip)\n",
    "    \n",
    "    # Bottleneck with dropout\n",
    "    x = conv_block_v2(x, base_filters * (2 ** depth), dropout=True)\n",
    "    \n",
    "    # Decoder path\n",
    "    for i in reversed(range(depth)):\n",
    "        x = decoder_block_v2(x, skips[i], base_filters * (2 ** i))\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='UNet_v2')\n",
    "    return model\n",
    "\n",
    "# Continuous Dice loss\n",
    "def dice_loss(y_true, y_pred, epsilon=1e-7):\n",
    "    \"\"\"Continuous Dice loss (1 - Dice coefficient).\"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    dice_coef = (2.0 * intersection + epsilon) / (K.sum(y_true_f) + K.sum(y_pred_f) + epsilon)\n",
    "    return 1.0 - dice_coef\n",
    "\n",
    "# Combined loss: BCE + Dice\n",
    "def combined_loss(y_true, y_pred):\n",
    "    \"\"\"Binary crossentropy + Dice loss.\"\"\"\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    bce = K.mean(bce)\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    return bce + dice\n",
    "\n",
    "# Build and compile U-Net v2\n",
    "unet_v2 = build_unet_v2(input_shape=(*IMG_SIZE, 3), base_filters=32, depth=4)\n",
    "unet_v2.summary()\n",
    "\n",
    "unet_v2.compile(\n",
    "    optimizer=Adam(learning_rate=LR),\n",
    "    loss=combined_loss,\n",
    "    metrics=[iou_metric, dice_metric]\n",
    ")\n",
    "print('U-Net v2 compiled with combined BCE+Dice loss.')\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_path_v2 = MODELS_DIR / 'unet_v2_best.keras'\n",
    "tb_log_dir_v2 = RUNS_DIR / 'tb' / 'v2'\n",
    "\n",
    "callbacks_v2 = [\n",
    "    ModelCheckpoint(\n",
    "        str(checkpoint_path_v2),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    TensorBoard(\n",
    "        log_dir=str(tb_log_dir_v2),\n",
    "        histogram_freq=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f'Checkpoint v2: {checkpoint_path_v2}')\n",
    "print(f'TensorBoard logs v2: {tb_log_dir_v2}')\n",
    "\n",
    "# Train U-Net v2\n",
    "history_v2 = unet_v2.fit(\n",
    "    train_ds,\n",
    "    validation_data=test_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_v2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot and save training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_v2.history['loss'], label='train_loss')\n",
    "axes[0].plot(history_v2.history['val_loss'], label='val_loss')\n",
    "axes[0].set_title('Loss (BCE+Dice)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].legend()\n",
    "\n",
    "# IoU\n",
    "axes[1].plot(history_v2.history['iou_metric'], label='train_iou')\n",
    "axes[1].plot(history_v2.history['val_iou_metric'], label='val_iou')\n",
    "axes[1].set_title('IoU')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].legend()\n",
    "\n",
    "# Dice\n",
    "axes[2].plot(history_v2.history['dice_metric'], label='train_dice')\n",
    "axes[2].plot(history_v2.history['val_dice_metric'], label='val_dice')\n",
    "axes[2].set_title('Dice')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "hist_path_v2 = RUNS_DIR / 'v2_history.png'\n",
    "plt.savefig(hist_path_v2, dpi=150, bbox_inches='tight')\n",
    "print(f'Saved U-Net v2 training history to {hist_path_v2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa71d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate U-Net v2 and compare with baseline\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load best v2 checkpoint\n",
    "best_model_path_v2 = MODELS_DIR / 'unet_v2_best.keras'\n",
    "model_eval_v2 = load_model(\n",
    "    str(best_model_path_v2), \n",
    "    custom_objects={\n",
    "        'iou_metric': iou_metric, \n",
    "        'dice_metric': dice_metric,\n",
    "        'combined_loss': combined_loss,\n",
    "        'dice_loss': dice_loss\n",
    "    }\n",
    ")\n",
    "print(f'Loaded U-Net v2 best model from {best_model_path_v2}')\n",
    "\n",
    "# Collect metrics per image for v2\n",
    "pa_list_v2 = []\n",
    "iou_list_v2 = []\n",
    "dice_list_v2 = []\n",
    "\n",
    "# Collect samples for qualitative visualization\n",
    "qual_samples_v2 = []\n",
    "\n",
    "for img_batch, gt_batch in test_ds:\n",
    "    # Predict\n",
    "    pred_batch = model_eval_v2.predict(img_batch, verbose=0)\n",
    "    \n",
    "    # Process each image in batch (batch=1 for test_ds)\n",
    "    for img, gt, pred in zip(img_batch.numpy(), gt_batch.numpy(), pred_batch):\n",
    "        # Threshold prediction at 0.5\n",
    "        pred_bin = (pred > 0.5).astype(np.float32)\n",
    "        gt_bin = (gt > 0.5).astype(np.float32)\n",
    "        \n",
    "        # Flatten for metric computation\n",
    "        gt_flat = gt_bin.flatten()\n",
    "        pred_flat = pred_bin.flatten()\n",
    "        \n",
    "        # Compute TP, TN, FP, FN\n",
    "        TP = np.sum((gt_flat == 1) & (pred_flat == 1))\n",
    "        TN = np.sum((gt_flat == 0) & (pred_flat == 0))\n",
    "        FP = np.sum((gt_flat == 0) & (pred_flat == 1))\n",
    "        FN = np.sum((gt_flat == 1) & (pred_flat == 0))\n",
    "        \n",
    "        # Pixel Accuracy\n",
    "        pa = (TP + TN) / (TP + TN + FP + FN + 1e-7)\n",
    "        \n",
    "        # IoU\n",
    "        iou = TP / (TP + FP + FN + 1e-7)\n",
    "        \n",
    "        # Dice\n",
    "        dice = (2 * TP) / (2 * TP + FP + FN + 1e-7)\n",
    "        \n",
    "        pa_list_v2.append(pa)\n",
    "        iou_list_v2.append(iou)\n",
    "        dice_list_v2.append(dice)\n",
    "        \n",
    "        # Store first 10 samples for visualization\n",
    "        if len(qual_samples_v2) < 10:\n",
    "            qual_samples_v2.append((img, gt.squeeze(), pred_bin.squeeze()))\n",
    "\n",
    "# Print U-Net v2 metrics\n",
    "print(f'\\n=== U-Net v2 Evaluation (test set, n={len(pa_list_v2)}) ===')\n",
    "print(f'Pixel Accuracy: {np.mean(pa_list_v2):.4f} ± {np.std(pa_list_v2):.4f}')\n",
    "print(f'IoU:            {np.mean(iou_list_v2):.4f} ± {np.std(iou_list_v2):.4f}')\n",
    "print(f'Dice:           {np.mean(dice_list_v2):.4f} ± {np.std(dice_list_v2):.4f}')\n",
    "\n",
    "# Print comparison table\n",
    "print('\\n' + '='*50)\n",
    "print('                  COMPARISON')\n",
    "print('='*50)\n",
    "print(f'{\"Model\":<15} {\"PixelAcc\":<12} {\"IoU\":<12} {\"Dice\":<12}')\n",
    "print('-'*50)\n",
    "print(f'{\"Baseline\":<15} {np.mean(pa_list):<12.4f} {np.mean(iou_list):<12.4f} {np.mean(dice_list):<12.4f}')\n",
    "print(f'{\"U-Net v2\":<15} {np.mean(pa_list_v2):<12.4f} {np.mean(iou_list_v2):<12.4f} {np.mean(dice_list_v2):<12.4f}')\n",
    "print('='*50)\n",
    "\n",
    "# Save qualitative results for v2\n",
    "eval_dir_v2 = RUNS_DIR / 'eval' / 'v2'\n",
    "eval_dir_v2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(10, 4, figsize=(16, 40))\n",
    "for i, (img, gt, pred) in enumerate(qual_samples_v2):\n",
    "    # Image\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f'Sample {i+1}: Image')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[i, 1].imshow(gt, cmap='gray')\n",
    "    axes[i, 1].set_title(f'Sample {i+1}: GT')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axes[i, 2].imshow(pred, cmap='gray')\n",
    "    axes[i, 2].set_title(f'Sample {i+1}: Pred')\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Overlay: image + prediction\n",
    "    axes[i, 3].imshow(img)\n",
    "    axes[i, 3].imshow(pred, cmap='Reds', alpha=0.4)\n",
    "    axes[i, 3].set_title(f'Sample {i+1}: Overlay')\n",
    "    axes[i, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "qual_path_v2 = eval_dir_v2 / 'qualitative_results.png'\n",
    "plt.savefig(qual_path_v2, dpi=150, bbox_inches='tight')\n",
    "print(f'\\nSaved U-Net v2 qualitative results to {qual_path_v2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7507a",
   "metadata": {},
   "source": [
    "# Optional: Pratheepan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddb883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate on Pratheepan dataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if Pratheepan dataset exists\n",
    "PRATHEEPAN_ROOT = REPO_ROOT / 'data' / 'pratheepan'\n",
    "\n",
    "if PRATHEEPAN_ROOT.exists():\n",
    "    print(f'Pratheepan dataset found at {PRATHEEPAN_ROOT}')\n",
    "    \n",
    "    # Scan and match pairs\n",
    "    prath_imgs, prath_msks = scan_files(PRATHEEPAN_ROOT)\n",
    "    prath_pairs = match_pairs(prath_imgs, prath_msks)\n",
    "    \n",
    "    if len(prath_pairs) > 0:\n",
    "        print(f'Found {len(prath_pairs)} Pratheepan pairs')\n",
    "        \n",
    "        # Build dataset (batch=1 for evaluation)\n",
    "        prath_ds = make_ds([(Path(img), Path(msk)) for img, msk in prath_pairs], batch=1, shuffle=False)\n",
    "        \n",
    "        # Load best baseline model (or use v2 if preferred)\n",
    "        from tensorflow.keras.models import load_model\n",
    "        best_model_path = MODELS_DIR / 'unet_best.keras'\n",
    "        model_prath = load_model(\n",
    "            str(best_model_path), \n",
    "            custom_objects={'iou_metric': iou_metric, 'dice_metric': dice_metric}\n",
    "        )\n",
    "        print(f'Loaded model from {best_model_path}')\n",
    "        \n",
    "        # Evaluate\n",
    "        pa_list_prath = []\n",
    "        iou_list_prath = []\n",
    "        dice_list_prath = []\n",
    "        qual_samples_prath = []\n",
    "        \n",
    "        for img_batch, gt_batch in prath_ds:\n",
    "            pred_batch = model_prath.predict(img_batch, verbose=0)\n",
    "            \n",
    "            for img, gt, pred in zip(img_batch.numpy(), gt_batch.numpy(), pred_batch):\n",
    "                # Threshold\n",
    "                pred_bin = (pred > 0.5).astype(np.float32)\n",
    "                gt_bin = (gt > 0.5).astype(np.float32)\n",
    "                \n",
    "                # Flatten\n",
    "                gt_flat = gt_bin.flatten()\n",
    "                pred_flat = pred_bin.flatten()\n",
    "                \n",
    "                # Compute metrics\n",
    "                TP = np.sum((gt_flat == 1) & (pred_flat == 1))\n",
    "                TN = np.sum((gt_flat == 0) & (pred_flat == 0))\n",
    "                FP = np.sum((gt_flat == 0) & (pred_flat == 1))\n",
    "                FN = np.sum((gt_flat == 1) & (pred_flat == 0))\n",
    "                \n",
    "                pa = (TP + TN) / (TP + TN + FP + FN + 1e-7)\n",
    "                iou = TP / (TP + FP + FN + 1e-7)\n",
    "                dice = (2 * TP) / (2 * TP + FP + FN + 1e-7)\n",
    "                \n",
    "                pa_list_prath.append(pa)\n",
    "                iou_list_prath.append(iou)\n",
    "                dice_list_prath.append(dice)\n",
    "                \n",
    "                # Store first 10 samples\n",
    "                if len(qual_samples_prath) < 10:\n",
    "                    qual_samples_prath.append((img, gt.squeeze(), pred_bin.squeeze()))\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'\\n=== Pratheepan Evaluation (n={len(pa_list_prath)}) ===')\n",
    "        print(f'Pixel Accuracy: {np.mean(pa_list_prath):.4f} ± {np.std(pa_list_prath):.4f}')\n",
    "        print(f'IoU:            {np.mean(iou_list_prath):.4f} ± {np.std(iou_list_prath):.4f}')\n",
    "        print(f'Dice:           {np.mean(dice_list_prath):.4f} ± {np.std(dice_list_prath):.4f}')\n",
    "        \n",
    "        # Save qualitative results\n",
    "        eval_dir_prath = RUNS_DIR / 'eval' / 'pratheepan'\n",
    "        eval_dir_prath.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        n_samples = min(10, len(qual_samples_prath))\n",
    "        fig, axes = plt.subplots(n_samples, 4, figsize=(16, 4 * n_samples))\n",
    "        \n",
    "        # Handle single sample case (axes won't be 2D)\n",
    "        if n_samples == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, (img, gt, pred) in enumerate(qual_samples_prath[:n_samples]):\n",
    "            # Image\n",
    "            axes[i, 0].imshow(img)\n",
    "            axes[i, 0].set_title(f'Sample {i+1}: Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            axes[i, 1].imshow(gt, cmap='gray')\n",
    "            axes[i, 1].set_title(f'Sample {i+1}: GT')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            axes[i, 2].imshow(pred, cmap='gray')\n",
    "            axes[i, 2].set_title(f'Sample {i+1}: Pred')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            axes[i, 3].imshow(img)\n",
    "            axes[i, 3].imshow(pred, cmap='Reds', alpha=0.4)\n",
    "            axes[i, 3].set_title(f'Sample {i+1}: Overlay')\n",
    "            axes[i, 3].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        qual_path_prath = eval_dir_prath / 'qualitative_results.png'\n",
    "        plt.savefig(qual_path_prath, dpi=150, bbox_inches='tight')\n",
    "        print(f'Saved Pratheepan qualitative results to {qual_path_prath}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Skipping Pratheepan evaluation (no image-mask pairs found).')\n",
    "else:\n",
    "    print('Skipping Pratheepan evaluation (dataset not found).')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
